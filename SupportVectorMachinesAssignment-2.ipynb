{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "749c934a-44e0-4e1d-a372-bad109d4d645",
   "metadata": {},
   "source": [
    "#Q1\n",
    "\n",
    "In machine learning algorithms, kernel functions are a mathematical concept used to transform data into a higher-dimensional feature space to make it more amenable for analysis. Polynomial functions are a type of kernel function used for this purpose.\n",
    "\n",
    "The relationship between polynomial functions and kernel functions can be summarized as follows:\n",
    "\n",
    "Kernel Functions:\n",
    "\n",
    "Kernel functions are used in various machine learning algorithms, such as Support Vector Machines (SVM) and kernelized versions of algorithms like Principal Component Analysis (PCA) and the Perceptron.\n",
    "These functions allow the algorithms to operate in a higher-dimensional feature space without explicitly computing the transformed feature vectors.\n",
    "Polynomial Kernel:\n",
    "\n",
    "The polynomial kernel is a specific type of kernel function used in machine learning.\n",
    "It takes the form K(x, y) = (γ * (x · y) + r)^d, where γ, r, and d are parameters that control the behavior of the kernel.\n",
    "The polynomial kernel effectively computes the dot product between two data points after applying a polynomial transformation.\n",
    "The relationship is that the polynomial kernel is a specific example of a kernel function, and it involves using polynomial functions to map data into a higher-dimensional space. The choice of the degree (d) and other parameters in the polynomial kernel determines the complexity and expressiveness of the feature space transformation.\n",
    "\n",
    "The polynomial kernel is particularly useful when the decision boundary of a classification problem is non-linear. By increasing the degree of the polynomial, you can model more complex decision boundaries. However, it's essential to be cautious about overfitting, as higher-degree polynomials can make the model too complex and lead to poor generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "637e3d3a-f2c4-4ed3-9190-77c3fc9fb123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "#Q2\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "poly_svm = SVC(kernel='poly', degree=3, C=1.0, gamma='scale')\n",
    "\n",
    "poly_svm.fit(X_train, y_train)\n",
    "\n",
    "y_pred = poly_svm.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ceb288-80ee-442f-bfda-e4962af2004b",
   "metadata": {},
   "source": [
    "#Q3\n",
    "\n",
    "In Support Vector Regression (SVR), the parameter ε (epsilon) is part of the ε-insensitive loss function used to determine the tube or margin within which errors are not penalized. The tube represents a region around the regression line where errors smaller than ε are considered acceptable and do not contribute to the loss. If an error exceeds ε, it incurs a penalty in the loss function.\n",
    "\n",
    "The relationship between the value of ε and the number of support vectors in SVR is as follows:\n",
    "\n",
    "Larger Epsilon (ε):\n",
    "\n",
    "A larger value of ε increases the width of the tube or margin.\n",
    "With a wider margin, SVR becomes more tolerant of errors, allowing data points to deviate further from the regression line while still being considered acceptable.\n",
    "As a result, the SVR model is likely to have more support vectors when ε is larger because more data points may fall within the wider margin.\n",
    "Smaller Epsilon (ε):\n",
    "\n",
    "A smaller value of ε narrows the margin.\n",
    "With a narrower margin, SVR becomes less tolerant of errors and requires data points to be closer to the regression line for a smaller loss.\n",
    "Consequently, the SVR model is likely to have fewer support vectors when ε is smaller because only data points very close to the regression line can be considered support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb3c240-0c7b-4650-848c-026dd18378cf",
   "metadata": {},
   "source": [
    "#Q4\n",
    "\n",
    "The choice of kernel function, C parameter, epsilon (ε) parameter, and gamma (γ) parameter in Support Vector Regression (SVR) can significantly affect the model's performance. Here's an explanation of how each of these parameters influences SVR's performance:\n",
    "\n",
    "Kernel Function:\n",
    "\n",
    "The kernel function determines the mapping of the input data into a higher-dimensional feature space, allowing SVR to model non-linear relationships.\n",
    "Different kernel functions are suited to different types of data and relationships:\n",
    "Linear Kernel: Suitable for data with a linear relationship.\n",
    "Polynomial Kernel: Useful for capturing moderate non-linearity. The degree parameter controls the polynomial order.\n",
    "RBF (Radial Basis Function) Kernel: Effective for highly non-linear data. The gamma parameter controls the kernel's shape.\n",
    "Sigmoid Kernel: Used for data with sigmoidal behavior.\n",
    "The choice of kernel function depends on the problem at hand, and it affects how well SVR can fit the data.\n",
    "C Parameter:\n",
    "\n",
    "The C parameter controls the trade-off between fitting the training data and preventing overfitting.\n",
    "A smaller C allows for a larger margin but may tolerate more errors, leading to a simpler model (higher bias).\n",
    "A larger C enforces a smaller margin and aims to minimize training errors, which can lead to a more complex model (lower bias).\n",
    "The choice of C depends on the trade-off between model bias and variance. Cross-validation helps find an appropriate value.\n",
    "Epsilon (ε) Parameter:\n",
    "\n",
    "The epsilon parameter defines the width of the ε-insensitive tube around the regression line. Errors within this tube are not penalized in the loss function.\n",
    "A larger ε allows for larger deviations of data points from the regression line, resulting in a more flexible model.\n",
    "A smaller ε constrains data points to be closer to the regression line, leading to a more rigid model.\n",
    "The choice of ε depends on the desired tolerance for errors in the model and the noise level in the data.\n",
    "Gamma (γ) Parameter:\n",
    "\n",
    "The gamma parameter is used in some kernel functions, such as the RBF kernel, and it controls the shape and spread of the kernel.\n",
    "A smaller γ value makes the kernel wider and smoother, which is suitable for capturing broad patterns in the data.\n",
    "A larger γ value makes the kernel narrower and more peaked, which is better for capturing fine-grained, localized patterns.\n",
    "The choice of γ depends on the data's characteristics and the trade-off between model complexity and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd033875-f9c5-4276-bf4a-9e88856d80a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['trained_svc_classifier.pkl']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q5\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib \n",
    "\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "svc_classifier = SVC()\n",
    "svc_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svc_classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_svc = grid_search.best_estimator_\n",
    "\n",
    "best_svc.fit(X, y)\n",
    "\n",
    "joblib.dump(best_svc, 'trained_svc_classifier.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f698d3e-c91c-4ac7-9bd9-4e3043fc1caa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
